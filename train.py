# -*- coding: utf-8 -*-
"""TRAIN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14pBa7F7gSiedT5pXtlq19nowlXzco0Wb
"""

import torch
import time
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    pipeline
)
import evaluate
from fastapi import FastAPI
from google.colab import files

df = pd.read_csv('data2.csv')

# Load data
data = [{"text": f"Input: {i} → Output: {o}"} for i, o in zip(df['Input'], df['Output'])]
dataset = Dataset.from_list(data)
# Tokenizer and model
model_name = "distilgpt2"
token = "HUGGINGFACE_TOKEN"
tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
# Add the padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token  # or '[PAD]' if you prefer
model = AutoModelForCausalLM.from_pretrained(model_name, token=token)


def tokenize(example):
    tokenized = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(tokenize)

training_args = TrainingArguments(
    output_dir="./model_output",
    per_device_train_batch_size=4,  # adjust for your GPU memory
    num_train_epochs=3,
    logging_steps=10,
    save_steps=100,
    save_total_limit=1,
    evaluation_strategy="no",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)
if torch.cuda.is_available():
    model.to('cuda')
trainer.train()

model.save_pretrained("fantasy_item_model2")
tokenizer.save_pretrained("fantasy_item_model2")

!zip -r fantasy_item_model2.zip fantasy_item_model2 # Change the filename here to match the saved model directory and the download filename
from google.colab import files
files.download("fantasy_item_model2.zip")

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

model_path = "fantasy_item_model2"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

prompt = "Input: Wand, ice element → Output:"
result = generator(prompt, max_length=30, do_sample=True, top_k=50, top_p=0.95)
print(result[0]['generated_text'])

import evaluate

rouge = evaluate.load("rouge")

predictions = []
references = []

for idx, row in df.iterrows():
    prompt = f"Input: {row['Input']} → Output:"
    output = generator(prompt, max_length=30, do_sample=False)[0]['generated_text']
    predicted = output.replace(prompt, "").strip()

    predictions.append(predicted)
    references.append(row['Output'].strip())

results = rouge.compute(predictions=predictions, references=references)
print(results)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from fastapi import FastAPI
# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch
# import time
# 
# app = FastAPI()
# start_time = time.time()
# 
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 
# model = AutoModelForCausalLM.from_pretrained("fantasy_item_model2").to(device)
# tokenizer = AutoTokenizer.from_pretrained("fantasy_item_model2")
# 
# @app.get("/generate")
# def generate(prompt: str):
#     try:
#         inputs = tokenizer(prompt, return_tensors="pt").to(device)
#         outputs = model.generate(**inputs, max_new_tokens=20, do_sample=True, top_k=50, temperature=0.8)
#         result = tokenizer.decode(outputs[0], skip_special_tokens=True)
#         return {"output": result}
#     except Exception as e:
#         return {"error": str(e)}
# 
# @app.get("/status")
# def status():
#     uptime = time.time() - start_time
#     return {
#         "status": "running",
#         "model": "fantasy_item_model2",
#         "device": str(device),
#         "uptime": f"{uptime:.2f} seconds"
#     }
#

from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("shuvankar/fantasy-item-model2")
tokenizer = AutoTokenizer.from_pretrained("shuvankar/fantasy-item-model2")